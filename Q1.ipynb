{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('./yolov5')\n",
    "\n",
    "# Import YOLOv5 modules\n",
    "from models.experimental import attempt_load\n",
    "from utils.general import check_img_size, non_max_suppression\n",
    "from utils.torch_utils import select_device\n",
    "from utils.augmentations import letterbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: cannot change to '/home/ved_maurya/sem4/Software': No such file or directory\n",
      "YOLOv5 ðŸš€ 2025-4-9 Python-3.12.7 torch-2.6.0+cu124 CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients, 4.5 GFLOPs\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded yolov5n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded yolov5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients, 48.9 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded yolov5m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "YOLOv5l summary: 367 layers, 46533693 parameters, 0 gradients, 109.0 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded yolov5l\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "YOLOv5x summary: 444 layers, 86705005 parameters, 0 gradients, 205.5 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded yolov5x\n"
     ]
    }
   ],
   "source": [
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = select_device('0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Download and load all YOLOv5 model variants\n",
    "models = {}\n",
    "model_names = ['yolov5n', 'yolov5s', 'yolov5m', 'yolov5l', 'yolov5x']\n",
    "\n",
    "for model_name in model_names:\n",
    "    model_path = f'yolov5/weights/{model_name}.pt'\n",
    "    \n",
    "    # Download model if it doesn't exist\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs('yolov5/weights', exist_ok=True)\n",
    "        !wget -P yolov5/weights/ https://github.com/ultralytics/yolov5/releases/download/v6.2/{model_name}.pt\n",
    "    \n",
    "    # Load model\n",
    "    model = attempt_load(model_path, device=device)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    models[model_name] = model\n",
    "    print(f\"Loaded {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 100 test images\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Get list of test images\n",
    "test_images = [os.path.join('test_images', f) for f in os.listdir('test_images') \n",
    "               if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "# Make sure we have at least 100 images\n",
    "if len(test_images) < 100:\n",
    "    print(f\"Warning: Only {len(test_images)} images found. The assignment requires at least 100 images.\")\n",
    "    # If you don't have enough images, you can duplicate the existing ones\n",
    "    while len(test_images) < 100:\n",
    "        test_images.append(test_images[len(test_images) % len(test_images)])\n",
    "\n",
    "# Limit to exactly 100 images\n",
    "test_images = test_images[:100]\n",
    "print(f\"Using {len(test_images)} test images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for inference\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from yolov5.utils.augmentations import letterbox\n",
    "\n",
    "def preprocess_image(img_path, img_size=640, device='cpu', half=False):\n",
    "    \"\"\"\n",
    "    Preprocess an image for YOLOv5 inference.\n",
    "\n",
    "    Args:\n",
    "        img_path (str): Path to the image file.\n",
    "        img_size (int): Target size for YOLOv5 model input.\n",
    "        device (str): Device to move the image to (\"cpu\" or \"cuda\").\n",
    "        half (bool): Whether to convert to float16.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Preprocessed image tensor of shape (1, 3, H, W)\n",
    "    \"\"\"\n",
    "    # Load image as RGB\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "    # Convert to NumPy array\n",
    "    img = np.array(img)\n",
    "\n",
    "    # Resize and pad\n",
    "    img = letterbox(img, new_shape=img_size)[0]\n",
    "\n",
    "    # HWC to CHW\n",
    "    img = img.transpose(2, 0, 1)\n",
    "\n",
    "    # Normalize and convert to tensor\n",
    "    img = np.ascontiguousarray(img, dtype=np.float32) / 255.0\n",
    "    img = torch.from_numpy(img).unsqueeze(0).to(device)\n",
    "\n",
    "    if half:\n",
    "        img = img.half()\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def run_inference(model, img_tensor, conf_thres=0.25, iou_thres=0.45):\n",
    "    \"\"\"\n",
    "    Run YOLOv5 inference and apply non-max suppression.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The YOLOv5 model.\n",
    "        img_tensor (torch.Tensor): Preprocessed input image (1, 3, H, W).\n",
    "        conf_thres (float): Confidence threshold.\n",
    "        iou_thres (float): IoU threshold for NMS.\n",
    "\n",
    "    Returns:\n",
    "        list: Detections after NMS.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        output = model(img_tensor)[0]  # [0] is needed for YOLOv5-style models\n",
    "\n",
    "        # Apply NMS\n",
    "        detections = non_max_suppression(output, conf_thres=conf_thres, iou_thres=iou_thres)\n",
    "\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model_name, model, image_paths, num_runs=1):\n",
    "    \"\"\"\n",
    "    Benchmark a model on multiple images\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model\n",
    "        model: PyTorch model\n",
    "        image_paths: List of paths to test images\n",
    "        num_runs: Number of benchmark runs\n",
    "        \n",
    "    Returns:\n",
    "        Average latency per image (ms)\n",
    "        Throughput (FPS)\n",
    "    \"\"\"\n",
    "    img_size = 640  # Default image size for YOLOv5\n",
    "    latencies = []\n",
    "    \n",
    "    # Warmup runs (important for accurate benchmarking)\n",
    "    print(f\"Warming up {model_name}...\")\n",
    "    for _ in range(5):\n",
    "        img = preprocess_image(image_paths[0], img_size)\n",
    "        _ = run_inference(model, img)\n",
    "    \n",
    "    # Perform benchmark runs\n",
    "    for run in range(num_runs):\n",
    "        print(f\"Benchmark run {run+1}/{num_runs} for {model_name}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for img_path in tqdm(image_paths, desc=f\"Processing images\"):\n",
    "            img = preprocess_image(img_path, img_size)\n",
    "            _ = run_inference(model, img)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_time = end_time - start_time\n",
    "        avg_latency_ms = (total_time * 1000) / len(image_paths)  # Convert to milliseconds\n",
    "        throughput_fps = len(image_paths) / total_time  # Frames per second\n",
    "        \n",
    "        latencies.append((avg_latency_ms, throughput_fps))\n",
    "        \n",
    "        print(f\"Run {run+1}: Latency = {avg_latency_ms:.2f} ms, FPS = {throughput_fps:.2f}\")\n",
    "    \n",
    "    # Calculate average results across runs\n",
    "    avg_latency = np.mean([l[0] for l in latencies])\n",
    "    avg_fps = np.mean([l[1] for l in latencies])\n",
    "    \n",
    "    return avg_latency, avg_fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Benchmarking yolov5n =====\n",
      "Warming up yolov5n...\n",
      "Benchmark run 1/1 for yolov5n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:05<00:00, 18.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: Latency = 53.27 ms, FPS = 18.77\n",
      "\n",
      "Final results for yolov5n:\n",
      "Average Latency: 53.27 ms\n",
      "Average Throughput: 18.77 FPS\n",
      "\n",
      "===== Benchmarking yolov5s =====\n",
      "Warming up yolov5s...\n",
      "Benchmark run 1/1 for yolov5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:11<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: Latency = 113.95 ms, FPS = 8.78\n",
      "\n",
      "Final results for yolov5s:\n",
      "Average Latency: 113.95 ms\n",
      "Average Throughput: 8.78 FPS\n",
      "\n",
      "===== Benchmarking yolov5m =====\n",
      "Warming up yolov5m...\n",
      "Benchmark run 1/1 for yolov5m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:29<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: Latency = 297.69 ms, FPS = 3.36\n",
      "\n",
      "Final results for yolov5m:\n",
      "Average Latency: 297.69 ms\n",
      "Average Throughput: 3.36 FPS\n",
      "\n",
      "===== Benchmarking yolov5l =====\n",
      "Warming up yolov5l...\n",
      "Benchmark run 1/1 for yolov5l\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:59<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: Latency = 591.16 ms, FPS = 1.69\n",
      "\n",
      "Final results for yolov5l:\n",
      "Average Latency: 591.16 ms\n",
      "Average Throughput: 1.69 FPS\n",
      "\n",
      "===== Benchmarking yolov5x =====\n",
      "Warming up yolov5x...\n",
      "Benchmark run 1/1 for yolov5x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:44<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: Latency = 1044.80 ms, FPS = 0.96\n",
      "\n",
      "Final results for yolov5x:\n",
      "Average Latency: 1044.80 ms\n",
      "Average Throughput: 0.96 FPS\n",
      "\n",
      "===== Benchmark Results =====\n",
      "         Latency (ms)        FPS\n",
      "yolov5n     53.272398  18.771447\n",
      "yolov5s    113.950291   8.775756\n",
      "yolov5m    297.689378   3.359206\n",
      "yolov5l    591.162312   1.691583\n",
      "yolov5x   1044.802024   0.957119\n"
     ]
    }
   ],
   "source": [
    "# Run benchmarks for all models\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n===== Benchmarking {model_name} =====\")\n",
    "    latency, fps = benchmark_model(model_name, model, test_images)\n",
    "    results[model_name] = {'Latency (ms)': latency, 'FPS': fps}\n",
    "    print(f\"\\nFinal results for {model_name}:\")\n",
    "    print(f\"Average Latency: {latency:.2f} ms\")\n",
    "    print(f\"Average Throughput: {fps:.2f} FPS\")\n",
    "    \n",
    "# Create result table as required by the assignment\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "print(\"\\n===== Benchmark Results =====\")\n",
    "print(results_df)\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv('yolov5_benchmark_results.csv')\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot latency\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(results_df.index, results_df['Latency (ms)'])\n",
    "plt.title('Inference Latency (ms)')\n",
    "plt.ylabel('Milliseconds')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot throughput\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(results_df.index, results_df['FPS'])\n",
    "plt.title('Throughput (FPS)')\n",
    "plt.ylabel('Frames Per Second')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('yolov5_benchmark_results.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (c10::Half) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m sample_img \u001b[38;5;241m=\u001b[39m test_images[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mvisualize_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m, in \u001b[0;36mvisualize_detection\u001b[0;34m(img_path, model, model_name)\u001b[0m\n\u001b[1;32m      8\u001b[0m orig_img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(orig_img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Draw bounding boxes\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pred[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[8], line 57\u001b[0m, in \u001b[0;36mrun_inference\u001b[0;34m(model, img_tensor, conf_thres, iou_thres)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mRun YOLOv5 inference and apply non-max suppression.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    list: Detections after NMS.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# [0] is needed for YOLOv5-style models\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Apply NMS\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     detections \u001b[38;5;241m=\u001b[39m non_max_suppression(output, conf_thres\u001b[38;5;241m=\u001b[39mconf_thres, iou_thres\u001b[38;5;241m=\u001b[39miou_thres)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/sem4/Software Programming for performance/YOLOv5-Roofline-Study/yolov5/models/yolo.py:270\u001b[0m, in \u001b[0;36mDetectionModel.forward\u001b[0;34m(self, x, augment, profile, visualize)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_augment(x)  \u001b[38;5;66;03m# augmented inference, None\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sem4/Software Programming for performance/YOLOv5-Roofline-Study/yolov5/models/yolo.py:169\u001b[0m, in \u001b[0;36mBaseModel._forward_once\u001b[0;34m(self, x, profile, visualize)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 169\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    170\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/sem4/Software Programming for performance/YOLOv5-Roofline-Study/yolov5/models/common.py:91\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     90\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Applies a fused convolution and activation function to the input tensor `x`.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (c10::Half) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "# Function to visualize detection on an image\n",
    "def visualize_detection(img_path, model, model_name):\n",
    "    import cv2\n",
    "    \n",
    "    # Preprocess image\n",
    "    img = preprocess_image(img_path).half()  # Convert input to FP16\n",
    "    orig_img = cv2.imread(img_path)\n",
    "    orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Run inference\n",
    "    pred = run_inference(model, img)\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    if len(pred[0]) > 0:\n",
    "        for *xyxy, conf, cls in pred[0]:\n",
    "            # Convert tensors to integers\n",
    "            x1, y1, x2, y2 = [int(coord) for coord in xyxy]\n",
    "            \n",
    "            # Draw rectangle\n",
    "            cv2.rectangle(orig_img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            \n",
    "            # Add label\n",
    "            label = f\"{conf:.2f}\"\n",
    "            cv2.putText(orig_img, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    # Display image\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(orig_img)\n",
    "    plt.title(f\"{model_name} Detection Example\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a sample detection with each model\n",
    "sample_img = test_images[0]\n",
    "for model_name, model in models.items():\n",
    "    visualize_detection(sample_img, model, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchinfo in /home/ved_maurya/.local/lib/python3.12/site-packages (1.8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: cannot change to '/home/ved_maurya/sem4/Software': No such file or directory\n",
      "YOLOv5 ðŸš€ 2025-4-9 Python-3.12.7 torch-2.6.0+cu124 CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients, 4.5 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Model Information for yolov5n =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 1,867,405\n",
      "Model Size: 7.17 MB\n",
      "GFLOPS per inference: 4.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Model Information for yolov5s =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 7,225,885\n",
      "Model Size: 27.61 MB\n",
      "GFLOPS per inference: 16.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients, 48.9 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Model Information for yolov5m =====\n",
      "Total Parameters: 21,172,173\n",
      "Model Size: 80.82 MB\n",
      "GFLOPS per inference: 48.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "YOLOv5l summary: 367 layers, 46533693 parameters, 0 gradients, 109.0 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Model Information for yolov5l =====\n",
      "Total Parameters: 46,533,693\n",
      "Model Size: 177.59 MB\n",
      "GFLOPS per inference: 109.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "YOLOv5x summary: 444 layers, 86705005 parameters, 0 gradients, 205.5 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Model Information for yolov5x =====\n",
      "Total Parameters: 86,705,005\n",
      "Model Size: 330.85 MB\n",
      "GFLOPS per inference: 205.67\n",
      "\n",
      "===== Model Information =====\n",
      "         Total Parameters  Model Size (MB)  GFLOPS per inference\n",
      "yolov5n           1867405         7.166678              4.496060\n",
      "yolov5s           7225885        27.607780             16.485154\n",
      "yolov5m          21172173        80.824740             48.967356\n",
      "yolov5l          46533693       177.587478            109.145071\n",
      "yolov5x          86705005       330.845027            205.669052\n",
      "Estimating peak GFLOPS of the hardware...\n",
      "Estimated peak GFLOPS: 316.14\n",
      "Estimating peak memory bandwidth...\n",
      "Estimated peak memory bandwidth: 11.36 GB/s\n",
      "\n",
      "===== Roofline Analysis for yolov5n =====\n",
      "Warming up yolov5n...\n",
      "Benchmark run 1/1 for yolov5n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:05<00:00, 17.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: GFLOPS/sec = 78.74\n",
      "GFLOPS per inference: 4.50\n",
      "Actual GFLOPS/sec: 78.74\n",
      "Operational Intensity: 642.41 FLOPS/Byte\n",
      "Bound Type: Compute-bound\n",
      "Peak GFLOPS Utilization: 24.91%\n",
      "\n",
      "===== Roofline Analysis for yolov5s =====\n",
      "Warming up yolov5s...\n",
      "Benchmark run 1/1 for yolov5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:14<00:00,  6.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: GFLOPS/sec = 113.34\n",
      "GFLOPS per inference: 16.49\n",
      "Actual GFLOPS/sec: 113.34\n",
      "Operational Intensity: 611.45 FLOPS/Byte\n",
      "Bound Type: Compute-bound\n",
      "Peak GFLOPS Utilization: 35.85%\n",
      "\n",
      "===== Roofline Analysis for yolov5m =====\n",
      "Warming up yolov5m...\n",
      "Benchmark run 1/1 for yolov5m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:27<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: GFLOPS/sec = 177.45\n",
      "GFLOPS per inference: 48.97\n",
      "Actual GFLOPS/sec: 177.45\n",
      "Operational Intensity: 620.39 FLOPS/Byte\n",
      "Bound Type: Compute-bound\n",
      "Peak GFLOPS Utilization: 56.13%\n",
      "\n",
      "===== Roofline Analysis for yolov5l =====\n",
      "Warming up yolov5l...\n",
      "Benchmark run 1/1 for yolov5l\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:53<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: GFLOPS/sec = 204.55\n",
      "GFLOPS per inference: 109.15\n",
      "Actual GFLOPS/sec: 204.55\n",
      "Operational Intensity: 629.35 FLOPS/Byte\n",
      "Bound Type: Compute-bound\n",
      "Peak GFLOPS Utilization: 64.70%\n",
      "\n",
      "===== Roofline Analysis for yolov5x =====\n",
      "Warming up yolov5x...\n",
      "Benchmark run 1/1 for yolov5x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:30<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: GFLOPS/sec = 227.04\n",
      "GFLOPS per inference: 205.67\n",
      "Actual GFLOPS/sec: 227.04\n",
      "Operational Intensity: 636.57 FLOPS/Byte\n",
      "Bound Type: Compute-bound\n",
      "Peak GFLOPS Utilization: 71.82%\n",
      "\n",
      "===== Roofline Analysis Results =====\n",
      "         GFLOPS per inference  Actual GFLOPS/sec  \\\n",
      "yolov5n              4.496060          78.743310   \n",
      "yolov5s             16.485154         113.343251   \n",
      "yolov5m             48.967356         177.452033   \n",
      "yolov5l            109.145071         204.553289   \n",
      "yolov5x            205.669052         227.043043   \n",
      "\n",
      "         Operational Intensity (FLOPS/Byte)     Bound Type  \\\n",
      "yolov5n                          642.412728  Compute-bound   \n",
      "yolov5s                          611.450773  Compute-bound   \n",
      "yolov5m                          620.386435  Compute-bound   \n",
      "yolov5l                          629.349290  Compute-bound   \n",
      "yolov5x                          636.567251  Compute-bound   \n",
      "\n",
      "         Peak GFLOPS Utilization (%)  \n",
      "yolov5n                    24.907408  \n",
      "yolov5s                    35.851765  \n",
      "yolov5m                    56.130104  \n",
      "yolov5l                    64.702541  \n",
      "yolov5x                    71.816307  \n",
      "\n",
      "===== Per-Layer Analysis for yolov5n =====\n",
      "Top 5 compute-intensive layers:\n",
      "1. Conv2d: 4.4681 GFLOPs, OI: 53.94\n",
      "2. SiLU: 0.0000 GFLOPs, OI: 0.00\n",
      "3. Conv: 0.0000 GFLOPs, OI: 0.00\n",
      "4. Bottleneck: 0.0000 GFLOPs, OI: 0.00\n",
      "5. Sequential: 0.0000 GFLOPs, OI: 0.00\n",
      "\n",
      "Highest compute utilization: Conv2d (OI: 53.94)\n",
      "Lowest compute utilization: SiLU (OI: 0.00)\n",
      "\n",
      "===== Per-Layer Analysis for yolov5s =====\n",
      "Top 5 compute-intensive layers:\n",
      "1. Conv2d: 16.4336 GFLOPs, OI: 106.30\n",
      "2. SiLU: 0.0000 GFLOPs, OI: 0.00\n",
      "3. Conv: 0.0000 GFLOPs, OI: 0.00\n",
      "4. Bottleneck: 0.0000 GFLOPs, OI: 0.00\n",
      "5. Sequential: 0.0000 GFLOPs, OI: 0.00\n",
      "\n",
      "Highest compute utilization: Conv2d (OI: 106.30)\n",
      "Lowest compute utilization: SiLU (OI: 0.00)\n",
      "\n",
      "===== Per-Layer Analysis for yolov5m =====\n",
      "Top 5 compute-intensive layers:\n",
      "1. Conv2d: 48.8724 GFLOPs, OI: 178.67\n",
      "2. SiLU: 0.0000 GFLOPs, OI: 0.00\n",
      "3. Conv: 0.0000 GFLOPs, OI: 0.00\n",
      "4. Bottleneck: 0.0000 GFLOPs, OI: 0.00\n",
      "5. Sequential: 0.0000 GFLOPs, OI: 0.00\n",
      "\n",
      "Highest compute utilization: Conv2d (OI: 178.67)\n",
      "Lowest compute utilization: SiLU (OI: 0.00)\n",
      "\n",
      "===== Per-Layer Analysis for yolov5l =====\n",
      "Top 5 compute-intensive layers:\n",
      "1. Conv2d: 108.9937 GFLOPs, OI: 253.37\n",
      "2. SiLU: 0.0000 GFLOPs, OI: 0.00\n",
      "3. Conv: 0.0000 GFLOPs, OI: 0.00\n",
      "4. Bottleneck: 0.0000 GFLOPs, OI: 0.00\n",
      "5. Sequential: 0.0000 GFLOPs, OI: 0.00\n",
      "\n",
      "Highest compute utilization: Conv2d (OI: 253.37)\n",
      "Lowest compute utilization: SiLU (OI: 0.00)\n",
      "\n",
      "===== Per-Layer Analysis for yolov5x =====\n",
      "Top 5 compute-intensive layers:\n",
      "1. Conv2d: 205.4482 GFLOPs, OI: 329.23\n",
      "2. SiLU: 0.0000 GFLOPs, OI: 0.00\n",
      "3. Conv: 0.0000 GFLOPs, OI: 0.00\n",
      "4. Bottleneck: 0.0000 GFLOPs, OI: 0.00\n",
      "5. Sequential: 0.0000 GFLOPs, OI: 0.00\n",
      "\n",
      "Highest compute utilization: Conv2d (OI: 329.23)\n",
      "Lowest compute utilization: SiLU (OI: 0.00)\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Calculate model parameters, size, and GFLOPs\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "# Import YOLOv5 modules (assuming you've already run your first notebook)\n",
    "sys.path.append('./yolov5')\n",
    "from models.experimental import attempt_load\n",
    "from utils.general import check_img_size\n",
    "from utils.torch_utils import select_device\n",
    "\n",
    "# Set device\n",
    "device = select_device('0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load models and collect model information\n",
    "models = {}\n",
    "model_names = ['yolov5n', 'yolov5s', 'yolov5m', 'yolov5l', 'yolov5x']\n",
    "model_info = {}\n",
    "\n",
    "def get_model_size_mb(model):\n",
    "    \"\"\"Calculate model size in MB\"\"\"\n",
    "    # Save model to a temporary file to get its size\n",
    "    temp_path = 'temp_model.pt'\n",
    "    torch.save(model.state_dict(), temp_path)\n",
    "    size_bytes = os.path.getsize(temp_path)\n",
    "    size_mb = size_bytes / (1024 * 1024)  # Convert bytes to MB\n",
    "    os.remove(temp_path)  # Clean up\n",
    "    return size_mb\n",
    "\n",
    "for model_name in model_names:\n",
    "    model_path = f'yolov5/weights/{model_name}.pt'\n",
    "    \n",
    "    # Load model if not already done\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs('yolov5/weights', exist_ok=True)\n",
    "        !wget -P yolov5/weights/ https://github.com/ultralytics/yolov5/releases/download/v6.2/{model_name}.pt\n",
    "    \n",
    "    # Load model\n",
    "    model = attempt_load(model_path, device=device)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    models[model_name] = model\n",
    "    \n",
    "    # Get model info\n",
    "    print(f\"\\n===== Model Information for {model_name} =====\")\n",
    "    input_size = (1, 3, 640, 640)  # Standard YOLOv5 input size\n",
    "    model_summary = summary(model, input_size=input_size, verbose=0)\n",
    "    \n",
    "    # Extract information\n",
    "    total_params = model_summary.total_params\n",
    "    model_size = get_model_size_mb(model)\n",
    "    \n",
    "    # The macs (multiply-accumulate operations) need to be converted to FLOPS\n",
    "    # 1 mac = 2 FLOPS (1 multiplication + 1 addition)\n",
    "    # and then convert to GFLOPS (divide by 10^9)\n",
    "    gflops = model_summary.total_mult_adds * 2 / 1e9\n",
    "    \n",
    "    model_info[model_name] = {\n",
    "        'Total Parameters': total_params,\n",
    "        'Model Size (MB)': model_size,\n",
    "        'GFLOPS per inference': gflops\n",
    "    }\n",
    "    \n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"GFLOPS per inference: {gflops:.2f}\")\n",
    "\n",
    "# Create a table with model information\n",
    "info_df = pd.DataFrame.from_dict(model_info, orient='index')\n",
    "print(\"\\n===== Model Information =====\")\n",
    "print(info_df)\n",
    "\n",
    "# Save model information to CSV\n",
    "info_df.to_csv('yolov5_model_info.csv')\n",
    "\n",
    "# Part 2: Determine if models are compute-bound or memory-bound\n",
    "\n",
    "# First, let's rerun the benchmark to measure actual GFLOPS/sec\n",
    "# Load the test images from the previous run\n",
    "test_images = [os.path.join('test_images', f) for f in os.listdir('test_images') \n",
    "               if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "test_images = test_images[:100]  # Limit to 100 images\n",
    "\n",
    "# Reuse preprocessing and inference functions from the previous notebook\n",
    "def preprocess_image(img_path, img_size=640):\n",
    "    \"\"\"Preprocess an image for YOLOv5 inference\"\"\"\n",
    "    # Load and convert to RGB\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "    # Convert to numpy array first\n",
    "    img = np.array(img)\n",
    "    \n",
    "    # Resize and pad\n",
    "    img = letterbox(img, img_size, stride=32)[0]\n",
    "    \n",
    "    # Convert from HWC to CHW format (height, width, channels) -> (channels, height, width)\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    \n",
    "    # Make contiguous in memory\n",
    "    img = np.ascontiguousarray(img)\n",
    "    \n",
    "    # Convert to PyTorch tensor and move to device\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    \n",
    "    # Normalize pixel values to 0-1\n",
    "    img = img.float() / 255.0\n",
    "    \n",
    "    # Add batch dimension if needed\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "        \n",
    "    return img\n",
    "\n",
    "def run_inference(model, img):\n",
    "    \"\"\"Run inference with a preprocessed image\"\"\"\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        # Forward pass through model\n",
    "        pred = model(img)[0]\n",
    "        \n",
    "        # Apply non-maximum suppression to filter detections\n",
    "        pred = non_max_suppression(pred)\n",
    "        \n",
    "    return pred\n",
    "\n",
    "# Benchmark function that measures GFLOPS/sec\n",
    "def benchmark_model_gflops(model_name, model, gflops_per_inference, image_paths, num_runs=1):\n",
    "    \"\"\"\n",
    "    Benchmark a model and calculate GFLOPS/sec\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model\n",
    "        model: PyTorch model\n",
    "        gflops_per_inference: GFLOPS required for one inference\n",
    "        image_paths: List of paths to test images\n",
    "        num_runs: Number of benchmark runs\n",
    "        \n",
    "    Returns:\n",
    "        Average GFLOPS/sec\n",
    "    \"\"\"\n",
    "    img_size = 640  # Default image size for YOLOv5\n",
    "    gflops_per_sec_list = []\n",
    "    \n",
    "    # Warmup runs\n",
    "    print(f\"Warming up {model_name}...\")\n",
    "    for _ in range(5):\n",
    "        img = preprocess_image(image_paths[0], img_size)\n",
    "        _ = run_inference(model, img)\n",
    "    \n",
    "    # Perform benchmark runs\n",
    "    for run in range(num_runs):\n",
    "        print(f\"Benchmark run {run+1}/{num_runs} for {model_name}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for img_path in tqdm(image_paths, desc=f\"Processing images\"):\n",
    "            img = preprocess_image(img_path, img_size)\n",
    "            _ = run_inference(model, img)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total_time = end_time - start_time\n",
    "        total_gflops = gflops_per_inference * len(image_paths)\n",
    "        gflops_per_sec = total_gflops / total_time\n",
    "        \n",
    "        gflops_per_sec_list.append(gflops_per_sec)\n",
    "        \n",
    "        print(f\"Run {run+1}: GFLOPS/sec = {gflops_per_sec:.2f}\")\n",
    "    \n",
    "    # Calculate average results across runs\n",
    "    avg_gflops_per_sec = np.mean(gflops_per_sec_list)\n",
    "    \n",
    "    return avg_gflops_per_sec\n",
    "\n",
    "# Estimate peak GFLOPS of the hardware\n",
    "def estimate_peak_gflops():\n",
    "    \"\"\"Estimate peak GFLOPS based on simple matrix multiplication benchmark\"\"\"\n",
    "    # For CPU, this is a rough estimate\n",
    "    print(\"Estimating peak GFLOPS of the hardware...\")\n",
    "    \n",
    "    # Create large matrices for multiplication\n",
    "    matrix_size = 2000\n",
    "    a = torch.randn(matrix_size, matrix_size, device=device)\n",
    "    b = torch.randn(matrix_size, matrix_size, device=device)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(3):\n",
    "        _ = torch.matmul(a, b)\n",
    "    \n",
    "    # Benchmark\n",
    "    n_iter = 5\n",
    "    start_time = time.time()\n",
    "    for _ in range(n_iter):\n",
    "        _ = torch.matmul(a, b)\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate GFLOPS (matrix multiplication is 2*n^3 operations)\n",
    "    elapsed_time = end_time - start_time\n",
    "    operations = 2 * matrix_size**3 * n_iter\n",
    "    peak_gflops = operations / elapsed_time / 1e9\n",
    "    \n",
    "    print(f\"Estimated peak GFLOPS: {peak_gflops:.2f}\")\n",
    "    return peak_gflops\n",
    "\n",
    "# Measure peak memory bandwidth\n",
    "def estimate_peak_memory_bandwidth():\n",
    "    \"\"\"Estimate peak memory bandwidth in GB/s\"\"\"\n",
    "    print(\"Estimating peak memory bandwidth...\")\n",
    "    \n",
    "    # Create large arrays for memory bandwidth test\n",
    "    array_size = 10**8  # 100M elements\n",
    "    a = torch.ones(array_size, device=device)\n",
    "    b = torch.ones(array_size, device=device)\n",
    "    c = torch.zeros(array_size, device=device)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(3):\n",
    "        c = a + b\n",
    "    \n",
    "    # Benchmark\n",
    "    n_iter = 10\n",
    "    start_time = time.time()\n",
    "    for _ in range(n_iter):\n",
    "        c = a + b\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate bandwidth (bytes read + bytes written per second)\n",
    "    elapsed_time = end_time - start_time\n",
    "    bytes_processed = (2 * a.element_size() + c.element_size()) * array_size * n_iter\n",
    "    bandwidth_gb_s = bytes_processed / elapsed_time / 1e9\n",
    "    \n",
    "    print(f\"Estimated peak memory bandwidth: {bandwidth_gb_s:.2f} GB/s\")\n",
    "    return bandwidth_gb_s\n",
    "\n",
    "# Determine whether the model is compute-bound or memory-bound\n",
    "# Get peak performance numbers\n",
    "peak_gflops = estimate_peak_gflops()\n",
    "peak_bandwidth = estimate_peak_memory_bandwidth()\n",
    "\n",
    "# Initialize results dictionary\n",
    "roofline_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n===== Roofline Analysis for {model_name} =====\")\n",
    "    \n",
    "    # Get model's GFLOPS per inference\n",
    "    gflops_per_inference = model_info[model_name]['GFLOPS per inference']\n",
    "    \n",
    "    # Get model's GFLOPS/sec from benchmarking\n",
    "    actual_gflops_per_sec = benchmark_model_gflops(\n",
    "        model_name, model, gflops_per_inference, test_images\n",
    "    )\n",
    "    \n",
    "    # Calculate operational intensity (FLOPS/Byte)\n",
    "    # We'll estimate this as GFLOPS / model size in GB\n",
    "    model_size_gb = model_info[model_name]['Model Size (MB)'] / 1024\n",
    "    operational_intensity = gflops_per_inference / model_size_gb\n",
    "    \n",
    "    # Calculate peak performance based on operational intensity\n",
    "    # If it's memory-bound, peak = operational_intensity * peak_bandwidth\n",
    "    # If it's compute-bound, peak = peak_gflops\n",
    "    compute_bound_peak = peak_gflops\n",
    "    memory_bound_peak = operational_intensity * peak_bandwidth\n",
    "    theoretical_peak = min(compute_bound_peak, memory_bound_peak)\n",
    "    \n",
    "    # Calculate utilization\n",
    "    utilization = actual_gflops_per_sec / peak_gflops * 100  # as percentage\n",
    "    \n",
    "    # Determine if the model is compute-bound or memory-bound\n",
    "    is_compute_bound = compute_bound_peak <= memory_bound_peak\n",
    "    bound_type = \"Compute-bound\" if is_compute_bound else \"Memory-bound\"\n",
    "    \n",
    "    # Store the results\n",
    "    roofline_results[model_name] = {\n",
    "        'GFLOPS per inference': gflops_per_inference,\n",
    "        'Actual GFLOPS/sec': actual_gflops_per_sec,\n",
    "        'Operational Intensity (FLOPS/Byte)': operational_intensity,\n",
    "        'Bound Type': bound_type,\n",
    "        'Peak GFLOPS Utilization (%)': utilization\n",
    "    }\n",
    "    \n",
    "    print(f\"GFLOPS per inference: {gflops_per_inference:.2f}\")\n",
    "    print(f\"Actual GFLOPS/sec: {actual_gflops_per_sec:.2f}\")\n",
    "    print(f\"Operational Intensity: {operational_intensity:.2f} FLOPS/Byte\")\n",
    "    print(f\"Bound Type: {bound_type}\")\n",
    "    print(f\"Peak GFLOPS Utilization: {utilization:.2f}%\")\n",
    "\n",
    "# Create a table with roofline analysis results\n",
    "roofline_df = pd.DataFrame.from_dict(roofline_results, orient='index')\n",
    "print(\"\\n===== Roofline Analysis Results =====\")\n",
    "print(roofline_df)\n",
    "\n",
    "# Save roofline analysis to CSV\n",
    "roofline_df.to_csv('yolov5_roofline_analysis.csv')\n",
    "\n",
    "# Visualize roofline model\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the roofline\n",
    "x_range = np.logspace(-1, 3, 100)  # Operational intensity range\n",
    "y_compute = np.ones_like(x_range) * peak_gflops\n",
    "y_memory = x_range * peak_bandwidth\n",
    "y_roof = np.minimum(y_compute, y_memory)\n",
    "\n",
    "plt.loglog(x_range, y_compute, 'b-', linewidth=2, label='Compute Roof')\n",
    "plt.loglog(x_range, y_memory, 'r-', linewidth=2, label='Memory Roof')\n",
    "plt.loglog(x_range, y_roof, 'k--', linewidth=2, label='Roofline')\n",
    "\n",
    "# Plot each model\n",
    "for model_name, info in roofline_results.items():\n",
    "    op_intensity = info['Operational Intensity (FLOPS/Byte)']\n",
    "    gflops_per_sec = info['Actual GFLOPS/sec']\n",
    "    plt.loglog(op_intensity, gflops_per_sec, 'o', markersize=10, label=model_name)\n",
    "\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
    "plt.xlabel('Operational Intensity (FLOPS/Byte)', fontsize=12)\n",
    "plt.ylabel('Performance (GFLOPS/sec)', fontsize=12)\n",
    "plt.title('Roofline Model for YOLOv5 Variants', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig('yolov5_roofline_model.png')\n",
    "plt.show()\n",
    "\n",
    "# Extra Credit: Per-layer analysis\n",
    "def analyze_layers(model, model_name):\n",
    "    \"\"\"Analyze the layers of a model to identify bottlenecks\"\"\"\n",
    "    print(f\"\\n===== Per-Layer Analysis for {model_name} =====\")\n",
    "    \n",
    "    # Prepare a sample input\n",
    "    input_tensor = torch.randn(1, 3, 640, 640).to(device)\n",
    "    \n",
    "    # Dictionary to store layer info\n",
    "    layer_info = {}\n",
    "    \n",
    "    # Register hooks to collect information for each layer\n",
    "    hooks = []\n",
    "    \n",
    "    def hook_fn(m, i, o):\n",
    "        # Skip non-compute layers\n",
    "        if isinstance(m, (torch.nn.BatchNorm2d, torch.nn.ReLU)):\n",
    "            return\n",
    "        \n",
    "        # Calculate input and output sizes\n",
    "        if isinstance(i, (tuple, list)) and len(i) > 0:\n",
    "            input_size = sum(inp.numel() * inp.element_size() for inp in i if isinstance(inp, torch.Tensor))\n",
    "        elif isinstance(i, torch.Tensor):\n",
    "            input_size = i.numel() * i.element_size()\n",
    "        else:\n",
    "            input_size = 0\n",
    "            \n",
    "        if isinstance(o, (tuple, list)):\n",
    "            output_size = sum(out.numel() * out.element_size() for out in o if isinstance(out, torch.Tensor))\n",
    "        elif isinstance(o, torch.Tensor):\n",
    "            output_size = o.numel() * o.element_size()\n",
    "        else:\n",
    "            output_size = 0\n",
    "        \n",
    "        memory_access = input_size + output_size  # bytes\n",
    "        \n",
    "        # Estimate FLOPs (very rough estimation)\n",
    "        flops = 0\n",
    "        if isinstance(m, torch.nn.Conv2d):\n",
    "            # For each output element: kernel_h * kernel_w * in_channels multiplications\n",
    "            # and kernel_h * kernel_w * in_channels - 1 additions\n",
    "            out_h, out_w = o.shape[2], o.shape[3]\n",
    "            kernel_h, kernel_w = m.kernel_size\n",
    "            in_channels = m.in_channels\n",
    "            out_channels = m.out_channels\n",
    "            groups = m.groups\n",
    "            \n",
    "            flops = 2 * out_h * out_w * kernel_h * kernel_w * in_channels * out_channels / groups\n",
    "        elif isinstance(m, torch.nn.Linear):\n",
    "            flops = 2 * m.in_features * m.out_features\n",
    "        \n",
    "        # Convert to GFLOPS\n",
    "        gflops = flops / 1e9\n",
    "        \n",
    "        # Calculate operational intensity\n",
    "        if memory_access > 0:\n",
    "            operational_intensity = flops / memory_access\n",
    "        else:\n",
    "            operational_intensity = 0\n",
    "        \n",
    "        # Store layer information\n",
    "        layer_name = str(m.__class__.__name__)\n",
    "        if layer_name not in layer_info:\n",
    "            layer_info[layer_name] = []\n",
    "        \n",
    "        layer_info[layer_name].append({\n",
    "            'GFLOPs': gflops,\n",
    "            'Memory Access (MB)': memory_access / 1e6,\n",
    "            'Operational Intensity (FLOPS/Byte)': operational_intensity\n",
    "        })\n",
    "    \n",
    "    # Register hook for each module\n",
    "    for name, module in model.named_modules():\n",
    "        hooks.append(module.register_forward_hook(hook_fn))\n",
    "    \n",
    "    # Run a forward pass\n",
    "    with torch.no_grad():\n",
    "        _ = model(input_tensor)\n",
    "    \n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    # Aggregate information for each layer type\n",
    "    aggregated_info = {}\n",
    "    for layer_name, data in layer_info.items():\n",
    "        if not data:  # Skip empty lists\n",
    "            continue\n",
    "        \n",
    "        total_gflops = sum(item['GFLOPs'] for item in data)\n",
    "        total_memory = sum(item['Memory Access (MB)'] for item in data)\n",
    "        avg_oi = sum(item['Operational Intensity (FLOPS/Byte)'] for item in data) / len(data)\n",
    "        \n",
    "        aggregated_info[layer_name] = {\n",
    "            'Count': len(data),\n",
    "            'Total GFLOPs': total_gflops,\n",
    "            'Total Memory Access (MB)': total_memory,\n",
    "            'Average Operational Intensity': avg_oi\n",
    "        }\n",
    "    \n",
    "    # Sort layers by GFLOPs\n",
    "    sorted_layers = sorted(\n",
    "        aggregated_info.items(), \n",
    "        key=lambda x: x[1]['Total GFLOPs'], \n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # Print top 5 compute-intensive layers\n",
    "    print(\"Top 5 compute-intensive layers:\")\n",
    "    for i, (layer_name, info) in enumerate(sorted_layers[:5], 1):\n",
    "        print(f\"{i}. {layer_name}: {info['Total GFLOPs']:.4f} GFLOPs, OI: {info['Average Operational Intensity']:.2f}\")\n",
    "    \n",
    "    # Find layers with highest and lowest compute utilization\n",
    "    # Assuming compute utilization is proportional to operational intensity\n",
    "    max_oi_layer = max(aggregated_info.items(), key=lambda x: x[1]['Average Operational Intensity'])\n",
    "    min_oi_layer = min(aggregated_info.items(), key=lambda x: x[1]['Average Operational Intensity'])\n",
    "    \n",
    "    print(f\"\\nHighest compute utilization: {max_oi_layer[0]} (OI: {max_oi_layer[1]['Average Operational Intensity']:.2f})\")\n",
    "    print(f\"Lowest compute utilization: {min_oi_layer[0]} (OI: {min_oi_layer[1]['Average Operational Intensity']:.2f})\")\n",
    "    \n",
    "    return aggregated_info\n",
    "\n",
    "# Run per-layer analysis for each model (Extra Credit)\n",
    "layer_analysis = {}\n",
    "for model_name, model in models.items():\n",
    "    layer_analysis[model_name] = analyze_layers(model, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: cannot change to '/home/ved_maurya/sem4/Software': No such file or directory\n",
      "YOLOv5 ðŸš€ 2025-4-9 Python-3.12.7 torch-2.6.0+cu124 CPU\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cProfile analysis...\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Pipeline Timing Breakdown =====\n",
      "Average time over 20 images:\n",
      "Loading: 2.63 ms (2.2%)\n",
      "Preprocessing: 2.03 ms (1.7%)\n",
      "Inference: 117.07 ms (96.0%)\n",
      "Postprocessing: 0.16 ms (0.1%)\n",
      "Total: 121.89 ms\n",
      "\n",
      "===== PyTorch Profiler Results (Top 10 Operations) =====\n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     aten::conv2d         1.16%       1.011ms        82.60%      72.312ms       1.205ms            60  \n",
      "                aten::convolution         0.66%     575.811us        81.45%      71.300ms       1.188ms            60  \n",
      "               aten::_convolution         0.91%     794.962us        80.79%      70.725ms       1.179ms            60  \n",
      "         aten::mkldnn_convolution        76.93%      67.346ms        79.88%      69.930ms       1.165ms            60  \n",
      "                      aten::silu_         6.96%       6.092ms         6.96%       6.092ms     106.869us            57  \n",
      "                        aten::cat         4.04%       3.539ms         4.51%       3.949ms     197.461us            20  \n",
      "                 aten::max_pool2d         0.03%      27.229us         2.17%       1.900ms     633.171us             3  \n",
      "    aten::max_pool2d_with_indices         2.14%       1.872ms         2.14%       1.872ms     624.095us             3  \n",
      "                      aten::empty         1.60%       1.402ms         1.60%       1.402ms      10.384us           135  \n",
      "                    aten::resize_         1.01%     886.768us         1.01%     886.768us      13.436us            66  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 87.542ms\n",
      "\n",
      "Wed Apr  9 21:50:42 2025    profile_output\n",
      "\n",
      "         477557 function calls (430108 primitive calls) in 2.903 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 1821 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       20    0.000    0.000    2.433    0.122 1195146626.py:114(process_single_image)\n",
      "       20    0.001    0.000    2.340    0.117 1195146626.py:60(run_inference)\n",
      "  4473/21    0.016    0.000    2.103    0.100 module.py:1735(_wrapped_call_impl)\n",
      "  4473/21    0.039    0.000    2.103    0.100 module.py:1743(_call_impl)\n",
      "       21    0.000    0.000    2.102    0.100 yolo.py:266(forward)\n",
      "       21    0.008    0.000    2.052    0.098 yolo.py:161(_forward_once)\n",
      "     1197    0.007    0.000    1.696    0.001 common.py:89(forward_fuse)\n",
      "     1260    0.004    0.000    1.542    0.001 conv.py:553(forward)\n",
      "     1260    0.003    0.000    1.536    0.001 conv.py:536(_conv_forward)\n",
      "     1260    1.532    0.001    1.532    0.001 {built-in method torch.conv2d}\n",
      "      168    0.004    0.000    1.166    0.007 common.py:245(forward)\n",
      "      168    0.002    0.000    0.523    0.003 container.py:248(forward)\n",
      "      231    0.018    0.000    0.518    0.002 common.py:177(forward)\n",
      "       40    0.000    0.000    0.249    0.006 profiler.py:848(_transit_action)\n",
      "       21    0.000    0.000    0.247    0.012 {method 'run' of '_contextvars.Context' objects}\n",
      "       13    0.001    0.000    0.245    0.019 ioloop.py:742(_run_callback)\n",
      "       20    0.000    0.000    0.243    0.012 profiler.py:792(__exit__)\n",
      "       20    0.000    0.000    0.242    0.012 profiler.py:806(stop)\n",
      "       20    0.000    0.000    0.242    0.012 profiler.py:235(stop_trace)\n",
      "       20    0.000    0.000    0.242    0.012 profiler.py:363(__exit__)\n",
      "\n",
      "\n",
      "\n",
      "Analysis complete. Check the generated files for detailed results.\n"
     ]
    }
   ],
   "source": [
    "# YOLOv5 Code Profiling and Hotspot Analysis\n",
    "import os\n",
    "import time\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Make sure YOLOv5 is in path\n",
    "sys.path.append('./yolov5')\n",
    "\n",
    "# Import YOLOv5 modules\n",
    "from models.experimental import attempt_load\n",
    "from utils.general import check_img_size, non_max_suppression\n",
    "from utils.torch_utils import select_device\n",
    "from utils.augmentations import letterbox\n",
    "\n",
    "# ======= Part 1: Define the full pipeline with timing for each component =======\n",
    "\n",
    "def load_image(img_path):\n",
    "    \"\"\"Load an image from path\"\"\"\n",
    "    start_time = time.time()\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_np = np.array(img)\n",
    "    load_time = time.time() - start_time\n",
    "    return img_np, load_time\n",
    "\n",
    "def preprocess_image(img_np, img_size=640, device='cpu'):\n",
    "    \"\"\"Preprocess an image for YOLOv5 inference\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Resize and pad\n",
    "    img = letterbox(img_np, img_size, stride=32)[0]\n",
    "    \n",
    "    # Convert from HWC to CHW format\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    \n",
    "    # Make contiguous in memory\n",
    "    img = np.ascontiguousarray(img)\n",
    "    \n",
    "    # Convert to PyTorch tensor and move to device\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    \n",
    "    # Normalize pixel values to 0-1\n",
    "    img = img.float() / 255.0\n",
    "    \n",
    "    # Add batch dimension if needed\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "    \n",
    "    preprocess_time = time.time() - start_time\n",
    "    return img, preprocess_time\n",
    "\n",
    "def run_inference(model, img):\n",
    "    \"\"\"Run inference with PyTorch profiler for detailed analysis\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use PyTorch profiler to analyze the inference step\n",
    "    with torch.profiler.profile(\n",
    "        activities=[\n",
    "            torch.profiler.ProfilerActivity.CPU,\n",
    "            torch.profiler.ProfilerActivity.CUDA if torch.cuda.is_available() else torch.profiler.ProfilerActivity.CPU\n",
    "        ],\n",
    "        record_shapes=True,\n",
    "        with_stack=False\n",
    "    ) as prof:\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            # Forward pass through model\n",
    "            pred = model(img)[0]\n",
    "    \n",
    "    # Store the profiler results for later analysis\n",
    "    profiler_results = prof\n",
    "    \n",
    "    # Apply non-maximum suppression\n",
    "    pred = non_max_suppression(pred)\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    return pred, inference_time, profiler_results\n",
    "\n",
    "def postprocess_results(pred, orig_shape, img_shape):\n",
    "    \"\"\"Post-process predictions to original image coordinates\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Scale predictions back to original image size\n",
    "    # This is a simplified version - in a real pipeline, we'd scale boxes back to original image coordinates\n",
    "    results = []\n",
    "    \n",
    "    if len(pred[0]) > 0:\n",
    "        # Get scaling factors\n",
    "        scale_w = orig_shape[1] / img_shape[2]\n",
    "        scale_h = orig_shape[0] / img_shape[1]\n",
    "        \n",
    "        # Scale each prediction\n",
    "        for *xyxy, conf, cls in pred[0]:\n",
    "            x1, y1, x2, y2 = [int(coord.item()) for coord in xyxy]\n",
    "            x1, x2 = int(x1 * scale_w), int(x2 * scale_w)\n",
    "            y1, y2 = int(y1 * scale_h), int(y2 * scale_h)\n",
    "            \n",
    "            results.append({\n",
    "                'bbox': [x1, y1, x2, y2],\n",
    "                'confidence': conf.item(),\n",
    "                'class': int(cls.item())\n",
    "            })\n",
    "    \n",
    "    postprocess_time = time.time() - start_time\n",
    "    return results, postprocess_time\n",
    "\n",
    "def process_single_image(model, img_path, img_size=640, device='cpu'):\n",
    "    \"\"\"Process a single image through the full pipeline with timing\"\"\"\n",
    "    # Load image\n",
    "    img_np, load_time = load_image(img_path)\n",
    "    orig_shape = img_np.shape\n",
    "    \n",
    "    # Preprocess\n",
    "    img, preprocess_time = preprocess_image(img_np, img_size, device)\n",
    "    \n",
    "    # Inference\n",
    "    pred, inference_time, profiler_results = run_inference(model, img)\n",
    "    \n",
    "    # Postprocess\n",
    "    results, postprocess_time = postprocess_results(pred, orig_shape, img.shape)\n",
    "    \n",
    "    # Total time\n",
    "    total_time = load_time + preprocess_time + inference_time + postprocess_time\n",
    "    \n",
    "    # Return timing breakdown\n",
    "    timing = {\n",
    "        'load_time': load_time,\n",
    "        'preprocess_time': preprocess_time,\n",
    "        'inference_time': inference_time,\n",
    "        'postprocess_time': postprocess_time,\n",
    "        'total_time': total_time\n",
    "    }\n",
    "    \n",
    "    return results, timing, profiler_results\n",
    "\n",
    "# ======= Part 2: Setup profiling run =======\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to profile YOLOv5 inference pipeline\"\"\"\n",
    "    # Set device\n",
    "    device = select_device('0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load model (using yolov5s for profiling)\n",
    "    model_name = 'yolov5s'\n",
    "    model_path = f'yolov5/weights/{model_name}.pt'\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs('yolov5/weights', exist_ok=True)\n",
    "        !wget -P yolov5/weights/ https://github.com/ultralytics/yolov5/releases/download/v6.2/{model_name}.pt\n",
    "    \n",
    "    model = attempt_load(model_path, device=device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Get test images\n",
    "    test_images = [os.path.join('test_images', f) for f in os.listdir('test_images') \n",
    "                  if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    test_images = test_images[:20]  # Limit to 20 images for profiling\n",
    "    \n",
    "    if len(test_images) == 0:\n",
    "        print(\"No test images found. Please run the setup code from Problem 1 first.\")\n",
    "        return\n",
    "    \n",
    "    # Run profiling with detailed timing\n",
    "    all_timings = []\n",
    "    detailed_profiler = None\n",
    "    \n",
    "    for i, img_path in enumerate(test_images):\n",
    "        results, timing, profiler = process_single_image(model, img_path, device=device)\n",
    "        all_timings.append(timing)\n",
    "        \n",
    "        # Store detailed profiler results for the first image only to avoid too much data\n",
    "        if i == 0:\n",
    "            detailed_profiler = profiler\n",
    "    \n",
    "    # Calculate average timings\n",
    "    avg_timings = {\n",
    "        'load_time': np.mean([t['load_time'] for t in all_timings]),\n",
    "        'preprocess_time': np.mean([t['preprocess_time'] for t in all_timings]),\n",
    "        'inference_time': np.mean([t['inference_time'] for t in all_timings]),\n",
    "        'postprocess_time': np.mean([t['postprocess_time'] for t in all_timings]),\n",
    "        'total_time': np.mean([t['total_time'] for t in all_timings])\n",
    "    }\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total = avg_timings['total_time']\n",
    "    percentages = {\n",
    "        'load_time': (avg_timings['load_time'] / total) * 100,\n",
    "        'preprocess_time': (avg_timings['preprocess_time'] / total) * 100,\n",
    "        'inference_time': (avg_timings['inference_time'] / total) * 100,\n",
    "        'postprocess_time': (avg_timings['postprocess_time'] / total) * 100\n",
    "    }\n",
    "    \n",
    "    # Report results\n",
    "    print(\"\\n===== Pipeline Timing Breakdown =====\")\n",
    "    print(f\"Average time over {len(test_images)} images:\")\n",
    "    print(f\"Loading: {avg_timings['load_time']*1000:.2f} ms ({percentages['load_time']:.1f}%)\")\n",
    "    print(f\"Preprocessing: {avg_timings['preprocess_time']*1000:.2f} ms ({percentages['preprocess_time']:.1f}%)\")\n",
    "    print(f\"Inference: {avg_timings['inference_time']*1000:.2f} ms ({percentages['inference_time']:.1f}%)\")\n",
    "    print(f\"Postprocessing: {avg_timings['postprocess_time']*1000:.2f} ms ({percentages['postprocess_time']:.1f}%)\")\n",
    "    print(f\"Total: {avg_timings['total_time']*1000:.2f} ms\")\n",
    "    \n",
    "    # Visualize the breakdown\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    components = ['Loading', 'Preprocessing', 'Inference', 'Postprocessing']\n",
    "    values = [percentages['load_time'], percentages['preprocess_time'], \n",
    "              percentages['inference_time'], percentages['postprocess_time']]\n",
    "    \n",
    "    plt.bar(components, values)\n",
    "    plt.title('Time Breakdown by Pipeline Component (%)')\n",
    "    plt.ylabel('Percentage of Total Time')\n",
    "    plt.savefig('pipeline_breakdown.png')\n",
    "    \n",
    "    # Print PyTorch profiler results\n",
    "    print(\"\\n===== PyTorch Profiler Results (Top 10 Operations) =====\")\n",
    "    print(detailed_profiler.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "    \n",
    "    # Export CPU vs GPU time if available\n",
    "    if torch.cuda.is_available():\n",
    "        cpu_time = detailed_profiler.key_averages().total_average().cpu_time\n",
    "        cuda_time = detailed_profiler.key_averages().total_average().cuda_time\n",
    "        print(f\"\\nCPU Time: {cpu_time:.2f} us\")\n",
    "        print(f\"GPU Time: {cuda_time:.2f} us\")\n",
    "        print(f\"CPU/GPU Time Ratio: {cpu_time/cuda_time if cuda_time else 'N/A'}\")\n",
    "    \n",
    "    # Save full profiler results to file\n",
    "    with open('pytorch_profiler_results.txt', 'w') as f:\n",
    "        f.write(detailed_profiler.key_averages().table(sort_by=\"cpu_time_total\"))\n",
    "    \n",
    "    return \"Profiling complete\"\n",
    "\n",
    "# ======= Part 3: Run the profiling =======\n",
    "\n",
    "# Method 1: Use cProfile\n",
    "print(\"Running cProfile analysis...\")\n",
    "cProfile.run('main()', 'profile_output')\n",
    "\n",
    "# Print formatted cProfile results\n",
    "p = pstats.Stats('profile_output')\n",
    "p.strip_dirs().sort_stats('cumulative').print_stats(20)  # Print top 20 functions by cumulative time\n",
    "\n",
    "# Save detailed cProfile results\n",
    "with open('cprofile_results.txt', 'w') as f:\n",
    "    stats = pstats.Stats('profile_output', stream=f)\n",
    "    stats.strip_dirs().sort_stats('cumulative').print_stats()\n",
    "\n",
    "# Create a human-readable summary from cProfile\n",
    "s = io.StringIO()\n",
    "ps = pstats.Stats('profile_output', stream=s).strip_dirs().sort_stats('cumulative')\n",
    "ps.print_stats(30)  # Top 30 functions\n",
    "cprofile_output = s.getvalue()\n",
    "\n",
    "# Parse cProfile output to extract key information\n",
    "def parse_cprofile_output(output):\n",
    "    lines = output.split('\\n')\n",
    "    parsed_data = []\n",
    "    \n",
    "    # Skip header lines\n",
    "    data_lines = [line for line in lines if line.strip() and not line.startswith('ncalls') and not line.startswith('   ')]\n",
    "    \n",
    "    for line in data_lines:\n",
    "        if '{method' in line or '<built-in' in line:\n",
    "            continue\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) >= 5:\n",
    "            try:\n",
    "                function_name = ' '.join(parts[5:])\n",
    "                if '/' in parts[0]:  # Handle case with primitive/total calls\n",
    "                    total_calls = parts[0].split('/')[0]\n",
    "                else:\n",
    "                    total_calls = parts[0]\n",
    "                \n",
    "                total_time = float(parts[1])\n",
    "                per_call = float(parts[2])\n",
    "                cumulative = float(parts[3])\n",
    "                \n",
    "                parsed_data.append({\n",
    "                    'function': function_name,\n",
    "                    'calls': total_calls,\n",
    "                    'total_time': total_time,\n",
    "                    'per_call': per_call,\n",
    "                    'cumulative': cumulative\n",
    "                })\n",
    "            except (ValueError, IndexError):\n",
    "                pass  # Skip lines that don't match expected format\n",
    "    \n",
    "    return parsed_data\n",
    "\n",
    "parsed_cprofile = parse_cprofile_output(cprofile_output)\n",
    "\n",
    "# Create summary visualization of cProfile results\n",
    "if parsed_cprofile:\n",
    "    # Get top 10 functions by total time\n",
    "    top_functions = sorted(parsed_cprofile, key=lambda x: x['total_time'], reverse=True)[:10]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh([f['function'][:50] for f in top_functions], [f['total_time'] for f in top_functions])\n",
    "    plt.xlabel('Total Time (seconds)')\n",
    "    plt.title('Top 10 Time-Consuming Functions (cProfile)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cprofile_top_functions.png')\n",
    "    \n",
    "    # Save parsed data to CSV\n",
    "    df = pd.DataFrame(parsed_cprofile)\n",
    "    df.to_csv('cprofile_functions.csv', index=False)\n",
    "\n",
    "print(\"\\nAnalysis complete. Check the generated files for detailed results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "# FIXED VERSION: Guaranteed consistent size preprocessing\n",
    "def preprocess_image_fixed(img_path, img_size=640, to_half=False):\n",
    "    \"\"\"\n",
    "    Preprocess an image with guaranteed consistent dimensions\n",
    "    \"\"\"\n",
    "    # Read image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not read image: {img_path}\")\n",
    "    \n",
    "    # Convert BGR to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Resize and pad to square with consistent dimensions\n",
    "    h, w = img.shape[:2]\n",
    "    r = img_size / max(h, w)  # Resize ratio\n",
    "    if r != 1:  # Only resize if necessary\n",
    "        interp = cv2.INTER_LINEAR\n",
    "        new_size = (int(w * r), int(h * r))\n",
    "        img = cv2.resize(img, new_size, interpolation=interp)\n",
    "    \n",
    "    # Create square canvas\n",
    "    new_img = np.full((img_size, img_size, 3), 114, dtype=np.uint8)\n",
    "    \n",
    "    # Center image on canvas\n",
    "    h, w = img.shape[:2]\n",
    "    offset_h, offset_w = (img_size - h) // 2, (img_size - w) // 2\n",
    "    new_img[offset_h:offset_h + h, offset_w:offset_w + w] = img\n",
    "    \n",
    "    # Convert HWC to CHW format\n",
    "    img = new_img.transpose(2, 0, 1)\n",
    "    \n",
    "    # Convert to tensor and normalize\n",
    "    img = torch.from_numpy(img).float() / 255.0\n",
    "    \n",
    "    # Add batch dimension\n",
    "    img = img.unsqueeze(0)\n",
    "    \n",
    "    # Convert to half precision if requested\n",
    "    if to_half:\n",
    "        img = img.half()\n",
    "        \n",
    "    return img\n",
    "\n",
    "# Proper inference function\n",
    "def run_inference_fixed(model, img, conf_thres=0.25, iou_thres=0.45):\n",
    "    \"\"\"Run inference with proper type handling\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        output = model(img)\n",
    "        \n",
    "        # Non-maximum suppression\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]  # Handle models that return multiple outputs\n",
    "        \n",
    "        pred = non_max_suppression(output, conf_thres=conf_thres, iou_thres=iou_thres)\n",
    "    \n",
    "    return pred\n",
    "\n",
    "# 1. BASELINE BENCHMARK\n",
    "def benchmark_baseline(model_name, model, image_paths, num_runs=3):\n",
    "    \"\"\"Benchmark baseline performance\"\"\"\n",
    "    model = model.float()  # Ensure model is in FP32\n",
    "    \n",
    "    total_time = 0\n",
    "    total_images = len(image_paths)\n",
    "    total_detections = 0\n",
    "    \n",
    "    # Warmup\n",
    "    print(f\"Warming up baseline {model_name}...\")\n",
    "    for img_path in image_paths[:3]:\n",
    "        img = preprocess_image_fixed(img_path)\n",
    "        _ = run_inference_fixed(model, img)\n",
    "    \n",
    "    # Benchmark\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        for img_path in tqdm(image_paths, desc=f\"Baseline {model_name}\", leave=False):\n",
    "            img = preprocess_image_fixed(img_path)\n",
    "            pred = run_inference_fixed(model, img)\n",
    "            \n",
    "            # Count detections\n",
    "            for det in pred:\n",
    "                total_detections += len(det)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_time = total_time / (total_images * num_runs)\n",
    "    fps = (total_images * num_runs) / total_time\n",
    "    avg_detections = total_detections / (total_images * num_runs)\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'total_time': total_time,\n",
    "        'avg_time_per_image': avg_time,\n",
    "        'fps': fps,\n",
    "        'avg_detections': avg_detections,\n",
    "        'optimization': 'Baseline'\n",
    "    }\n",
    "\n",
    "# 2. FP16 BENCHMARK\n",
    "def benchmark_fp16(model_name, base_model, image_paths, num_runs=3):\n",
    "    \"\"\"Benchmark FP16 performance (only beneficial with GPU)\"\"\"\n",
    "    # Check hardware FP16 support\n",
    "    has_gpu = torch.cuda.is_available()\n",
    "    \n",
    "    if has_gpu:\n",
    "        device = torch.device('cuda')\n",
    "        model = base_model.to(device).half()\n",
    "    else:\n",
    "        print(\"WARNING: Running FP16 on CPU may be slower than baseline FP32\")\n",
    "        model = base_model.half()\n",
    "    \n",
    "    total_time = 0\n",
    "    total_images = len(image_paths)\n",
    "    total_detections = 0\n",
    "    \n",
    "    # Warmup\n",
    "    print(f\"Warming up FP16 {model_name}...\")\n",
    "    for img_path in image_paths[:3]:\n",
    "        img = preprocess_image_fixed(img_path, to_half=True)\n",
    "        if has_gpu:\n",
    "            img = img.to(device)\n",
    "        _ = run_inference_fixed(model, img)\n",
    "    \n",
    "    # Benchmark\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        for img_path in tqdm(image_paths, desc=f\"FP16 {model_name}\", leave=False):\n",
    "            img = preprocess_image_fixed(img_path, to_half=True)\n",
    "            if has_gpu:\n",
    "                img = img.to(device)\n",
    "            pred = run_inference_fixed(model, img)\n",
    "            \n",
    "            # Count detections\n",
    "            for det in pred:\n",
    "                total_detections += len(det)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    return {\n",
    "        'model_name': f\"{model_name} (FP16)\",\n",
    "        'total_time': total_time,\n",
    "        'avg_time_per_image': total_time / (total_images * num_runs),\n",
    "        'fps': (total_images * num_runs) / total_time,\n",
    "        'avg_detections': total_detections / (total_images * num_runs),\n",
    "        'optimization': 'FP16'\n",
    "    }\n",
    "\n",
    "# 3. BATCH PROCESSING BENCHMARK\n",
    "def benchmark_batch(model_name, base_model, image_paths, batch_size=4, num_runs=3):\n",
    "    \"\"\"Benchmark batch processing\"\"\"\n",
    "    model = base_model.float()\n",
    "    \n",
    "    total_time = 0\n",
    "    total_images = len(image_paths)\n",
    "    total_detections = 0\n",
    "    \n",
    "    # Prepare batched data\n",
    "    print(f\"Preparing batches for {model_name}...\")\n",
    "    batches = []\n",
    "    \n",
    "    # Process full batches\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        if len(batch_paths) == batch_size:  # Only use full batches\n",
    "            # Pre-process all images to same size\n",
    "            batch_imgs = torch.cat([preprocess_image_fixed(path) for path in batch_paths], dim=0)\n",
    "            batches.append((batch_imgs, batch_paths))\n",
    "    \n",
    "    # Process individual images for remainder\n",
    "    remainder_paths = image_paths[len(batches) * batch_size:]\n",
    "    \n",
    "    # Warmup\n",
    "    print(f\"Warming up batch {model_name}...\")\n",
    "    if batches:\n",
    "        batch_imgs, _ = batches[0]\n",
    "        _ = model(batch_imgs)\n",
    "    \n",
    "    # Benchmark\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        # Process batches\n",
    "        for batch_imgs, batch_paths in tqdm(batches, desc=f\"Batch {model_name}\", leave=False):\n",
    "            with torch.no_grad():\n",
    "                # Forward pass\n",
    "                outputs = model(batch_imgs)\n",
    "                \n",
    "                # Apply NMS\n",
    "                preds = non_max_suppression(outputs)\n",
    "                \n",
    "                # Count detections\n",
    "                for det in preds:\n",
    "                    total_detections += len(det)\n",
    "        \n",
    "        # Process remainder individually\n",
    "        for img_path in remainder_paths:\n",
    "            img = preprocess_image_fixed(img_path)\n",
    "            pred = run_inference_fixed(model, img)\n",
    "            \n",
    "            for det in pred:\n",
    "                total_detections += len(det)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    # Account for remainder in metrics\n",
    "    processed_images = len(batches) * batch_size + len(remainder_paths)\n",
    "    \n",
    "    return {\n",
    "        'model_name': f\"{model_name} (Batch Size={batch_size})\",\n",
    "        'total_time': total_time,\n",
    "        'avg_time_per_image': total_time / (processed_images * num_runs),\n",
    "        'fps': (processed_images * num_runs) / total_time,\n",
    "        'avg_detections': total_detections / (processed_images * num_runs),\n",
    "        'optimization': 'Batch Processing'\n",
    "    }\n",
    "\n",
    "# 4. MULTI-THREADING BENCHMARK\n",
    "def process_single_image(model, img_path):\n",
    "    \"\"\"Process a single image for threading\"\"\"\n",
    "    try:\n",
    "        img = preprocess_image_fixed(img_path)\n",
    "        pred = run_inference_fixed(model, img)\n",
    "        \n",
    "        # Count detections\n",
    "        total = 0\n",
    "        for det in pred:\n",
    "            total += len(det)\n",
    "        \n",
    "        return total\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {e}\")\n",
    "        return 0\n",
    "\n",
    "def benchmark_threading(model_name, base_model, image_paths, num_threads=4, num_runs=3):\n",
    "    \"\"\"Benchmark multi-threading\"\"\"\n",
    "    # Clone model to avoid any shared state issues\n",
    "    model = base_model.float()\n",
    "    \n",
    "    # Calculate optimal thread count if not specified\n",
    "    if num_threads <= 0:\n",
    "        import multiprocessing\n",
    "        num_threads = min(multiprocessing.cpu_count(), 8)  # Limit to 8 max\n",
    "    \n",
    "    total_time = 0\n",
    "    total_images = len(image_paths)\n",
    "    total_detections = 0\n",
    "    \n",
    "    # Warmup\n",
    "    print(f\"Warming up threaded {model_name}...\")\n",
    "    process_single_image(model, image_paths[0])\n",
    "    \n",
    "    # Benchmark\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        print(f\"Running threaded inference (Run {run+1}/{num_runs})...\")\n",
    "        \n",
    "        # Process images in parallel\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "            # Map function to all images\n",
    "            futures = {executor.submit(process_single_image, model, path): path for path in image_paths}\n",
    "            \n",
    "            # Process results as they complete\n",
    "            completed = 0\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                detections = future.result()\n",
    "                total_detections += detections\n",
    "                completed += 1\n",
    "                \n",
    "                if completed % 10 == 0:\n",
    "                    print(f\"  Completed {completed}/{total_images} images\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    return {\n",
    "        'model_name': f\"{model_name} (Threads={num_threads})\",\n",
    "        'total_time': total_time,\n",
    "        'avg_time_per_image': total_time / (total_images * num_runs),\n",
    "        'fps': (total_images * num_runs) / total_time,\n",
    "        'avg_detections': total_detections / (total_images * num_runs),\n",
    "        'optimization': 'Threading'\n",
    "    }\n",
    "\n",
    "# Run all benchmarks\n",
    "def run_benchmarks(model_name, model, image_paths, num_runs=3, batch_size=4, num_threads=4):\n",
    "    \"\"\"Run all benchmarks and compare results\"\"\"\n",
    "    print(f\"\\n===== Performance Optimization Benchmarks for {model_name} =====\")\n",
    "    results = []\n",
    "    \n",
    "    # 1. Baseline\n",
    "    try:\n",
    "        baseline = benchmark_baseline(model_name, model, image_paths, num_runs)\n",
    "        results.append(baseline)\n",
    "    except Exception as e:\n",
    "        print(f\"Baseline benchmark failed: {e}\")\n",
    "    \n",
    "    # 2. FP16\n",
    "    try:\n",
    "        fp16 = benchmark_fp16(model_name, model, image_paths, num_runs)\n",
    "        results.append(fp16)\n",
    "    except Exception as e:\n",
    "        print(f\"FP16 benchmark failed: {e}\")\n",
    "    \n",
    "    # 3. Batch Processing\n",
    "    try:\n",
    "        batch = benchmark_batch(model_name, model, image_paths, batch_size, num_runs)\n",
    "        results.append(batch)\n",
    "    except Exception as e:\n",
    "        print(f\"Batch processing benchmark failed: {e}\")\n",
    "    \n",
    "    # 4. Threading\n",
    "    try:\n",
    "        threaded = benchmark_threading(model_name, model, image_paths, num_threads, num_runs)\n",
    "        results.append(threaded)\n",
    "    except Exception as e:\n",
    "        print(f\"Threading benchmark failed: {e}\")\n",
    "    \n",
    "    # Create DataFrame and calculate speedups\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        # Sort by optimization type\n",
    "        df = df.sort_values('optimization')\n",
    "        \n",
    "        # Calculate speedups relative to baseline\n",
    "        baseline_fps = df[df['optimization'] == 'Baseline']['fps'].values[0]\n",
    "        df['speedup'] = df['fps'] / baseline_fps\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\n===== Optimization Results =====\")\n",
    "        print(df[['model_name', 'avg_time_per_image', 'fps', 'avg_detections', 'speedup']])\n",
    "        \n",
    "        # Plot results\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(df['model_name'], df['fps'])\n",
    "        plt.title('Performance Comparison (FPS)')\n",
    "        plt.ylabel('Frames per Second')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{model_name}_fps_comparison.png')\n",
    "        plt.show()\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(\"No successful benchmarks to report.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Performance Optimization Benchmarks for yolov5s =====\n",
      "Warming up baseline yolov5s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Running FP16 on CPU may be slower than baseline FP32\n",
      "Warming up FP16 yolov5s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing batches for yolov5s...\n",
      "Warming up batch yolov5s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up threaded yolov5s...\n",
      "Running threaded inference (Run 1/1)...\n",
      "  Completed 10/20 images\n",
      "  Completed 20/20 images\n",
      "\n",
      "===== Optimization Results =====\n",
      "               model_name  avg_time_per_image       fps  avg_detections  \\\n",
      "0                 yolov5s            0.436522  2.290837             4.9   \n",
      "2  yolov5s (Batch Size=4)            0.163529  6.115108             4.9   \n",
      "1          yolov5s (FP16)           11.643311  0.085886             4.9   \n",
      "3     yolov5s (Threads=4)            0.117560  8.506299             4.9   \n",
      "\n",
      "    speedup  \n",
      "0  1.000000  \n",
      "2  2.669376  \n",
      "1  0.037491  \n",
      "3  3.713183  \n"
     ]
    }
   ],
   "source": [
    "# Define benchmark parameters\n",
    "model_name = 'yolov5s'\n",
    "model = models[model_name]\n",
    "num_runs = 1  # Increase for more reliable results\n",
    "batch_size = 4  # Experiment with different batch sizes\n",
    "num_threads = 4  # Set to number of CPU cores for best results\n",
    "\n",
    "# Define test images if not already defined\n",
    "if 'test_images' not in locals():\n",
    "    test_images = [os.path.join('test_images', f) for f in os.listdir('test_images') \n",
    "                   if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    # Ensure at least 100 images are available\n",
    "    if len(test_images) < 100:\n",
    "        while len(test_images) < 100:\n",
    "            test_images.append(test_images[len(test_images) % len(test_images)])\n",
    "    test_images = test_images[:100]\n",
    "\n",
    "# Use a smaller set of images for faster testing\n",
    "benchmark_images = test_images[:20]\n",
    "\n",
    "# Run all benchmarks\n",
    "results = run_benchmarks(\n",
    "    model_name, \n",
    "    model, \n",
    "    benchmark_images, \n",
    "    num_runs=num_runs,\n",
    "    batch_size=batch_size,\n",
    "    num_threads=num_threads\n",
    ")\n",
    "\n",
    "# Save results\n",
    "if results is not None:\n",
    "    results.to_csv(f'{model_name}_optimization_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
